{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baby Name Generator Using Character-Level Neural Network Language Model \n",
    "\n",
    "### Introduction\n",
    "\n",
    "This scary looking figure comes from a [2003 paper](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) that uses a Multi-layer Perceptron for language modeling. It shows a neural network that takes as input a sequence of words and outputs a probability distribution over the next word in the sequence. I will be putting a twist on this: I will instead create a Language Model (LM) that takes as input a sequence of characters and outputs a probability distribution over the next character in the sequence. This will allow us to generate baby names!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset \n",
    "\n",
    "I will be using a relatively small dataset, consisting of tens of thousands of baby names. Since the text is, well, non-numeric, we will have to numericalize our data in some way.\n",
    "\n",
    "Since we're working on a character-level language model, we will be creating encodings for each and every character, hence the *vocabulary* is only as large as the English alphabet.\n",
    "\n",
    "We will need to maintain a record of *indices*: these will help us convert from characters to numbers and vice versa. Note that we will also include a special character `[.]` to denote the end of a name (since it is necessary for the model to be able to predict when to stop throwing out more characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
      "32033\n",
      "['zuber', 'zubeyr', 'zyell', 'zyheem', 'zykeem', 'zylas', 'zyran', 'zyrie', 'zyron', 'zzyzx']\n"
     ]
    }
   ],
   "source": [
    "with open(\"names.txt\") as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "print(words[:10])\n",
    "print(len(words))\n",
    "print((words[-10:]))\n",
    "# print([i for i in words if \"x\" in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = set(''.join(words)) # set of all characters in the corpus\n",
    "chars = sorted(list(chars)) # sorted list of all characters\n",
    "\n",
    "# creating a dictionary of char to index and index to char\n",
    "# Note that . is a special character that should be assigned index 0\n",
    "# The characters should start from 1\n",
    "stoi = {char: idx for idx, char in enumerate(chars, start=1)}\n",
    "itos = {idx: char for idx, char in enumerate(chars, start=1)}\n",
    "\n",
    "stoi['.'] = 0 # special character for EOS or SOS\n",
    "itos[0] = '.'\n",
    "\n",
    "\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset will be set up in the following way:\n",
    "\n",
    "- The input to the model will be a fixed-length sequence of characters, that we represent as integers for now. This is the **context vector**.\n",
    "\n",
    "- The ground truth will be the next character in the sequence, also represented as an integer.\n",
    "\n",
    "For example, take the name `olivia`. If we have the context length (`ctx_len`) equal $3$, then we can create multiple samples from this name:\n",
    "- `..o` $\\rightarrow$ `l`\n",
    "- `.ol` $\\rightarrow$ `i`\n",
    "- `oli` $\\rightarrow$ `v`\n",
    "- `liv` $\\rightarrow$ `i`\n",
    "- `ivi` $\\rightarrow$ `a`\n",
    "- `via` $\\rightarrow$ `.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3]) torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "ctx_len = 3\n",
    "X, Y = [], []\n",
    "\n",
    "# Loop through each of the words\n",
    "for word in words:\n",
    "\n",
    "    # vector of zeros of length ctx_len\n",
    "    ctx = [0] * ctx_len\n",
    "    \n",
    "    word = word + '.' # add a . at the end of the word\n",
    "    \n",
    "    # Looping through each character in the word\n",
    "    for char in word:\n",
    "\n",
    "        # Convert the character to an index\n",
    "        char_idx = stoi[char]\n",
    "\n",
    "        #  Append the context and the index to the X and Y lists respectively\n",
    "        X.append(ctx)\n",
    "        Y.append(char_idx)\n",
    "\n",
    "        # Update the context by removing the first character and adding the current character\n",
    "        ctx = ctx[1:] + [char_idx]\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "print(X.shape, Y.shape) # should be (228146,3), (228146)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... -> e\n",
      "..e -> m\n",
      ".em -> m\n",
      "emm -> a\n",
      "mma -> .\n",
      "... -> o\n",
      "..o -> l\n",
      ".ol -> i\n",
      "oli -> v\n",
      "liv -> i\n",
      "ivi -> a\n",
      "via -> .\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    \n",
    "    context_indices = X[i]\n",
    "    target_index = Y[i]\n",
    "#     print(context_indices)\n",
    "#     print(target_index)\n",
    "    context_chs = [itos[i.item()] for i in context_indices]\n",
    "    context_str = ''.join(context_chs)    \n",
    "    target_ch = itos[target_index.item()]\n",
    "    print(context_str, \"->\", target_ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Training a Model \n",
    "\n",
    "We will be using a simple Multilayer Perceptron to create our language model. The model will take in a sequence of characters and output a probability distribution over the next character in the sequence. We will be using an Embedding layer, `nn.Embedding`, to learn a representation for each character in our vocabulary. This will allow our model to learn a representation that is more suited to the task at hand.\n",
    "\n",
    "we can read more about Embeddings [here](https://en.wikipedia.org/wiki/Word_embedding) for starters, but for now, think of it as a way to *project* some identifier (whether that be an integer or a one-hot vector for a character index), into *some* vector space. This vector space is learned by the model, and is optimized for the task at hand. If we set the size of the embedding to be $d$, then the output of the embedding layer will be a vector of size $d$.\n",
    "\n",
    "Put simply, for the task at hand, Embeddings let us turn characters into learnable vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 27])\n"
     ]
    }
   ],
   "source": [
    "# Create the language model and perform a forward pass \n",
    "class MLPLM(nn.Module):\n",
    "    def __init__(self, vocab_size=27, emb_dim=2, ctx_len=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create an embedding layer to extract better features (vocab_size -> emb_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "        # Create a regular neural network (emb_dim*ctx_len -> ... -> vocab_size)\n",
    "        self.fc = nn.Sequential(nn.Linear(emb_dim * ctx_len, 200),nn.ReLU(),nn.Linear(200, vocab_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pass the input through the embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Flatten the input (batch_size, ctx_len, emb_dim) -> (batch_size, ctx_len*emb_dim)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Pass the flattened input through the feedforward network\n",
    "        return self.fc(x)\n",
    "\n",
    "model = MLPLM()\n",
    "\n",
    "# Dummy forward pass\n",
    "out = model(X)\n",
    "print(out.shape) # should be (228146, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train loss: 3.3127\n",
      "Epoch 50 | Train loss: 2.4753\n",
      "Epoch 100 | Train loss: 2.3380\n",
      "Epoch 150 | Train loss: 2.2843\n",
      "Epoch 200 | Train loss: 2.2506\n",
      "Epoch 250 | Train loss: 2.2308\n",
      "Epoch 300 | Train loss: 2.2191\n",
      "Epoch 350 | Train loss: 2.2059\n"
     ]
    }
   ],
   "source": [
    "# Training the model \n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def fit(model, X, Y, optimizer, loss_fn, epochs=200):\n",
    "    history = {\"train_loss\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = loss_fn(logits, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        if epoch % (epochs // 10) == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch} | Train loss: {train_loss:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "history = fit(model, X, Y, optimizer, loss_fn, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Names \n",
    "\n",
    "The algorithm I will be using is simple:\n",
    "\n",
    "1. Start with an vector of zeros (the model knows this means the start of a name from the training process)\n",
    "\n",
    "2. Feed this vector into the model to get a probability distribution over the next character\n",
    "\n",
    "3. Sample a character from this distribution\n",
    "\n",
    "4. Repeat steps 2 and 3 until we get a `[.]` character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fendi.\n",
      "ille.\n",
      "mohaneilan.\n",
      "betleelandrayya.\n",
      "avcleiel.\n",
      "mayza.\n",
      "ade.\n",
      "yre.\n",
      "nyson.\n",
      "gherynd.\n",
      "abtus.\n",
      "lua.\n",
      "isabdon.\n",
      "sunad.\n",
      "jahimpkaishiel.\n",
      "mirzin.\n",
      "darielanizae.\n",
      "wan.\n",
      "eeta.\n",
      "merdren.\n"
     ]
    }
   ],
   "source": [
    "num_names = 20\n",
    "\n",
    "for i in range(num_names):\n",
    "\n",
    "    # Create a vector of zeros for starters\n",
    "    ctx = [0, 0, 0]\n",
    "    gen_name = ''\n",
    "\n",
    "    # Infinite loop till we get a . character\n",
    "    while True:\n",
    "\n",
    "        # Convert the context to a tensor\n",
    "        ctx_tensor = torch.tensor(ctx).unsqueeze(0)\n",
    "\n",
    "        # Pass the context through the model\n",
    "        logits = model(ctx_tensor)\n",
    "\n",
    "        # Get the probabilities by applying softmax\n",
    "        probas = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        # Sample from the distribution to get the next character (use torch.multinomial)\n",
    "        idx = torch.multinomial(probas, 1)\n",
    "\n",
    "        # Convert the index to a character\n",
    "        char = itos[idx.item()]\n",
    "\n",
    "        # Append the character to the generated name\n",
    "        gen_name += char\n",
    "\n",
    "        # Update the context\n",
    "        ctx = ctx[1:] + [idx.item()]\n",
    "\n",
    "        # Break if we get a . character\n",
    "        if char == '.':  \n",
    "            break\n",
    "    \n",
    "    # Print the generated name!\n",
    "    print(gen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
